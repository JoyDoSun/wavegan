{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.nn.parallel\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "# import torchvision.datasets as dset\n",
    "# import torchvision.transforms as transforms\n",
    "# import torchvision.utils as vutils\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "import pescador\n",
    "import librosa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PhaseShuffle(nn.Module):\n",
    "    \"\"\"\n",
    "    Performs phase shuffling, i.e. shifting feature axis of a 3D tensor\n",
    "    by a random integer in {-n, n} and performing reflection padding where\n",
    "    necessary\n",
    "    \"\"\"\n",
    "    def __init__(self, n):\n",
    "        super(PhaseShuffle, self).__init__()\n",
    "        self.n = n\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Make sure to use PyTorch to generate number RNG state is all shared\n",
    "        k = int(torch.Tensor(1).random_(0,self.n + 1)) - 5\n",
    "        \n",
    "        # Return if no phase shift\n",
    "        if k == 0:\n",
    "            return x\n",
    "        \n",
    "        # Slice feature dimension\n",
    "        if k > 0:\n",
    "            x_trunc = x[:, :, :-k]\n",
    "            pad = (0, k)\n",
    "        else:\n",
    "            x_trunc = x[:, :, -k:]\n",
    "            pad = (-k, 0)\n",
    "        \n",
    "        # Reflection padding\n",
    "        x_shuffle = F.pad(x_trunc, pad, mode='reflect')\n",
    "        assert x_shuffle.shape == x.shape, \"{}, {}\".format(x_shuffle.shape, x.shape)\n",
    "        return x_shuffle\n",
    "        \n",
    "\n",
    "\n",
    "class WaveGANGenerator(nn.Module):\n",
    "    def __init__(self, d, ngpus, c=1, latent_dim=100, verbose=False):\n",
    "        super(WaveGANGenerator, self).__init__()\n",
    "        self.ngpus = ngpus\n",
    "        self.d = d\n",
    "        self.c = c\n",
    "        self.latent_dim = latent_dim\n",
    "        self.fc1 = nn.Linear(100, 256*d)\n",
    "        self.verbose = verbose\n",
    "        \n",
    "        self.tconv1 = nn.ConvTranspose1d(16*d, 8*d, 25, stride=4, padding=11, output_padding=1) \n",
    "        self.tconv2 = nn.ConvTranspose1d(8*d, 4*d, 25, stride=4, padding=11, output_padding=1) \n",
    "        self.tconv3 = nn.ConvTranspose1d(4*d, 2*d, 25, stride=4, padding=11, output_padding=1) \n",
    "        self.tconv4 = nn.ConvTranspose1d(2*d, d, 25, stride=4, padding=11, output_padding=1) \n",
    "        self.tconv5 = nn.ConvTranspose1d(d, c, 25, stride=4, padding=11, output_padding=1) \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "\n",
    "        x = x.view(-1, 16*self.d, 16)\n",
    "        if self.verbose:\n",
    "            print(x.shape)\n",
    "            \n",
    "        x = F.relu(self.tconv1(x))\n",
    "        if self.verbose:\n",
    "            print(x.shape)\n",
    "            \n",
    "        x = F.relu(self.tconv2(x))\n",
    "        if self.verbose:\n",
    "            print(x.shape)\n",
    "            \n",
    "        x = F.relu(self.tconv3(x))\n",
    "        if self.verbose:\n",
    "            print(x.shape)\n",
    "            \n",
    "        x = F.relu(self.tconv4(x))\n",
    "        if self.verbose:\n",
    "            print(x.shape)\n",
    "            \n",
    "        output = F.tanh(self.tconv5(x))\n",
    "        if self.verbose:\n",
    "            print(output.shape)\n",
    "\n",
    "        return output\n",
    "\n",
    "    \n",
    "class WaveGANDiscriminator(nn.Module):\n",
    "    def __init__(self, d, ngpus, c=1, n=2, verbose=False):\n",
    "        super(WaveGANDiscriminator, self).__init__()\n",
    "        self.d = d\n",
    "        self.ngpus = ngpus\n",
    "        self.c = c\n",
    "        self.n = n\n",
    "        self.verbose = verbose\n",
    "        # Conv2d(in_channels, out_channels, kernel_size, stride=1, etc.)\n",
    "        self.conv1 = nn.Conv1d(c, d, 25, stride=4, padding=11)\n",
    "        self.conv2 = nn.Conv1d(d, 2*d, 25, stride=4, padding=11)\n",
    "        self.conv3 = nn.Conv1d(2*d, 4*d, 25, stride=4, padding=11)\n",
    "        self.conv4 = nn.Conv1d(4*d, 8*d, 25, stride=4, padding=11)\n",
    "        self.conv5 = nn.Conv1d(8*d, 16*d, 25, stride=4, padding=11)\n",
    "        self.ps1 = PhaseShuffle(n)\n",
    "        self.ps2 = PhaseShuffle(n)\n",
    "        self.ps3 = PhaseShuffle(n)\n",
    "        self.ps4 = PhaseShuffle(n)\n",
    "        self.fc1 = nn.Linear(256*d, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.leaky_relu(self.conv1(x))\n",
    "        if self.verbose:\n",
    "            print(x.shape)\n",
    "        x = self.ps1(x)\n",
    "        \n",
    "        x = F.leaky_relu(self.conv2(x))\n",
    "        if self.verbose:\n",
    "            print(x.shape)            \n",
    "        x = self.ps2(x)\n",
    "        \n",
    "        x = F.leaky_relu(self.conv3(x))\n",
    "        if self.verbose:\n",
    "            print(x.shape)\n",
    "        x = self.ps3(x)\n",
    "        \n",
    "        x = F.leaky_relu(self.conv4(x))\n",
    "        if self.verbose:\n",
    "            print(x.shape)            \n",
    "        x = self.ps4(x)\n",
    "        \n",
    "        x = self.conv5(x)\n",
    "        if self.verbose:\n",
    "            print(x.shape)\n",
    "            \n",
    "        x = x.view(-1, 256*self.d)\n",
    "        if self.verbose:\n",
    "            print(x.shape)\n",
    "            \n",
    "        return F.sigmoid(self.fc1(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create generator\n",
    "latent_dim = 100\n",
    "gen = WaveGANGenerator(d=64, ngpus=0, c=1, latent_dim=latent_dim).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create discriminator\n",
    "disc = WaveGANDiscriminator(d=64, ngpus=0).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample from noise distribution p(z)\n",
    "z = torch.Tensor(5, latent_dim).uniform_(0, 1)\n",
    "z = Variable(z)\n",
    "z = z.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the sample through the generator to generate a sample\n",
    "# from the model distribution\n",
    "out = gen(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       " 0.4990\n",
       " 0.4990\n",
       " 0.4990\n",
       " 0.4990\n",
       " 0.4990\n",
       "[torch.cuda.FloatTensor of size 5x1 (GPU 0)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluate the given waveforms with the discriminator\n",
    "disc(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def file_sample_generator(filepath, window_length=16384, fs=16000):\n",
    "    \"\"\"\n",
    "    Audio sample generator\n",
    "    \"\"\"\n",
    "    audio_data = librosa.load(filepath, sr=fs)\n",
    "    audio_len = len(audio_data)\n",
    "    \n",
    "    # Pad audio to at least a single frame\n",
    "    if audio_len < window_length:\n",
    "        pad_length = window_length - audio_len\n",
    "        left_pad = pad_length // 2\n",
    "        right_pad = pad_length - left_pad\n",
    "        \n",
    "        audio_data = np.pad(audio_data, (left_pad, right_pad), mode='constant')\n",
    "        \n",
    "    while True:\n",
    "        if audio_len == window_length:\n",
    "            # If we only have a single frame's worth of audio, just yield the whole audio\n",
    "            sample = audio_data\n",
    "        else:\n",
    "            # Sample a random window from the audio file\n",
    "            start_idx = np.random.randint(0,len(audio_data) - window_length)\n",
    "            end_idx = start_idx + window_length\n",
    "            sample = audio_data[start_idx:end_idx]\n",
    "            \n",
    "        yield {'X': sample}\n",
    "    \n",
    "def create_batch_generator(audio_filepath_list, batch_size):\n",
    "    streamers = []\n",
    "    for audio_filepath in audio_filepath_list:\n",
    "        s = pescador.Streamer(file_sample_generator, audio_filepath)\n",
    "        streamers.append(s)\n",
    "        \n",
    "    mux = pescador.ShuffledMux(streamers)\n",
    "    batch_gen = pescador.buffer_stream(mux, batch_size)\n",
    "    \n",
    "    return batch_gen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adapted from https://github.com/caogang/wgan-gp/blob/master/gan_toy.py\n",
    "def calc_gradient_penalty(model_dis, real_data, fake_data, batch_size, lmbda):\n",
    "    # Compute interpolation factors\n",
    "    alpha = torch.rand(batch_size, 1)\n",
    "    alpha = alpha.expand(real_data.size())\n",
    "    alpha = alpha.cuda() if use_cuda else alpha\n",
    "\n",
    "    # Interpolate between real and fake data\n",
    "    interpolates = alpha * real_data + ((1 - alpha) * fake_data)\n",
    "    if use_cuda:\n",
    "        interpolates = interpolates.cuda()\n",
    "    interpolates = autograd.Variable(interpolates, requires_grad=True)\n",
    "\n",
    "    # Evaluate discriminator\n",
    "    disc_interpolates = model_dis(interpolates)\n",
    "\n",
    "    # Obtain gradients of the discriminator with respect to the inputs\n",
    "    gradients = autograd.grad(outputs=disc_interpolates, inputs=interpolates,\n",
    "                              grad_outputs=torch.ones(disc_interpolates.size()).cuda() if use_cuda else torch.ones(\n",
    "                                  disc_interpolates.size()),\n",
    "                              create_graph=True, retain_graph=True, only_inputs=True)[0]\n",
    "\n",
    "    # Compute MSE between 1.0 and the gradient of the norm penalty to encourage discriminator\n",
    "    # to be a 1-Lipschitz function\n",
    "    gradient_penalty = ((gradients.norm(2, dim=1) - 1) ** 2).mean() * lmbda\n",
    "    return gradient_penalty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adapted from https://github.com/caogang/wgan-gp/blob/master/gan_toy.py\n",
    "def train_wavegan(train_gen, valid_data, test_data, num_epochs, batches_per_epoch,\n",
    "                  batch_size, lmbda=0.1, ngpus=1, model_size=64, discriminator_updates=5, epochs_per_sample=10,\n",
    "                  sample_size=20, loss='wgan-gp'):\n",
    "    # TODO: Incorporate validation and test data\n",
    "    \n",
    "    # Build the models\n",
    "    model_gen = WaveGANGenerator(model_size, ngpus, c=1, lr=qe-4, beta_1=0.5, beta_2=0.9, latent_dim=100)\n",
    "    model_dis = WaveGANDiscriminator(model_size, ngpus, c=1, n=2)\n",
    "    \n",
    "    use_cuda = ngpus > 1\n",
    "    \n",
    "    # Convenient values for \n",
    "    one = torch.FloatTensor([1])\n",
    "    neg_one = one * -1\n",
    "    if use_cuda:\n",
    "        one = one.cuda()\n",
    "        neg_one = neg_one.cuda()\n",
    "    \n",
    "    # Initialize optimizers for each model\n",
    "    optimizer_gen = optim.Adam(model_gen.parameters(), lr=lr, betas=(beta_1, beta_2))\n",
    "    optimizer_dis = optim.Adam(model_dis.parameters(), lr=lr, betas=(beta_1, beta_2))\n",
    "\n",
    "    # Sample noise used for seeing the evolution of generated output samples throughout training\n",
    "    sample_noise = torch.randn(sample_size, 100)\n",
    "    if use_cuda:\n",
    "        noise = noise.cuda()\n",
    "    sample_noisev = autograd.Variable(sample_noise)\n",
    "    \n",
    "    samples = {}\n",
    "    history = []\n",
    "    \n",
    "    # Loop over the dataset multiple times\n",
    "    for epoch in range(num_epochs):\n",
    "\n",
    "        train_iter = iter(train_gen)\n",
    "        \n",
    "        epoch_history = []\n",
    "\n",
    "        for batch_idx in range(batches_per_epoch):\n",
    "\n",
    "            # Set model parameters to require gradients to be computed and stored\n",
    "            for p in model_dis.parameters():\n",
    "                p.requires_grad = True\n",
    "\n",
    "            # Initialize the metrics for this batch\n",
    "            batch_history = {\n",
    "                'discriminator': [],\n",
    "                'generator': {}\n",
    "            }\n",
    "            \n",
    "            # Discriminator Training Phase:\n",
    "            # -> Train discriminator k times\n",
    "            for iter_d in range(discriminator_updates):\n",
    "                # Get new batch of real training data\n",
    "                real_data = next(train_iter)\n",
    "                real_data = torch.Tensor(real_data)\n",
    "                if use_cuda:\n",
    "                    real_data = real_data.cuda()\n",
    "                real_data_v = autograd.Variable(real_data)\n",
    "\n",
    "                # Reset gradients\n",
    "                model_dis.zero_grad()\n",
    "\n",
    "                # a) Compute loss contribution from real training data and backprop\n",
    "                # (negative of the empirical mean, w.r.t. the data distribution, of the discr. output)\n",
    "                D_real = model_dis(real_data_v)\n",
    "                D_real = D_real.mean()\n",
    "                # Negate since we want to _maximize_ this quantity\n",
    "                D_real.backward(neg_one)\n",
    "\n",
    "                # b) Compute loss contribution from generated data and backprop\n",
    "                # (empirical mean, w.r.t. the generator distribution, of the discr. output)\n",
    "                # Generate noise in latent space\n",
    "                noise = torch.randn(batch_size, 2)\n",
    "                if use_cuda:\n",
    "                    noise = noise.cuda()\n",
    "                noisev = autograd.Variable(noise, volatile=True)  # totally freeze model_gen\n",
    "                # Generate data by passing noise through the generator\n",
    "                fake = autograd.Variable(model_gen(noisev, real_data_v).data)\n",
    "                inputv = fake\n",
    "                D_fake = model_dis(inputv)\n",
    "                D_fake = D_fake.mean()\n",
    "                D_fake.backward(one)\n",
    "\n",
    "                # c) Compute gradient penalty and backprop\n",
    "                gradient_penalty = calc_gradient_penalty(model_dis, real_data_v.data, fake.data, batch_size, lmbda)\n",
    "                gradient_penalty.backward()\n",
    "                \n",
    "                # d) Update the discriminator\n",
    "                optimizer_dis.step()\n",
    "                \n",
    "                # Compute metrics and record in batch history\n",
    "                D_cost = D_fake - D_real + gradient_penalty\n",
    "                Wasserstein_D = D_real - D_fake\n",
    "                batch_history['discriminator'].append({\n",
    "                    'cost': D_cost.data.numpy(),\n",
    "                    'wasserstein_cost': Wasserstein_D.data.numpy()\n",
    "                })\n",
    "\n",
    "            ############################\n",
    "            # (2) Update G network\n",
    "            ###########################\n",
    "            \n",
    "            # Prevent discriminator from computing gradients, since\n",
    "            # we are only updating the generator\n",
    "            for p in model_dis.parameters():\n",
    "                p.requires_grad = False\n",
    "\n",
    "            # Reset generator gradients\n",
    "            model_gen.zero_grad()\n",
    "\n",
    "            # Sample from the generator\n",
    "            noise = torch.randn(batch_size, 100)\n",
    "            if use_cuda:\n",
    "                noise = noise.cuda()\n",
    "            noisev = autograd.Variable(noise)\n",
    "            fake = model_gen(noisev)\n",
    "            \n",
    "            # Compute generator loss and backprop\n",
    "            # (negative of empirical mean (w.r.t generator distribution) of discriminator output)\n",
    "            G = model_dis(fake)\n",
    "            G = G.mean()\n",
    "            G.backward(neg_one)\n",
    "            G_cost = -G\n",
    "\n",
    "            # Update generator\n",
    "            optimizer_gen.step()\n",
    "\n",
    "            # Record generator loss\n",
    "            batch_history['generator']['cost'] = G_cost.data.numpy()\n",
    "            \n",
    "            # Record batch metrics\n",
    "            epoch_history.append(batch_history)\n",
    "            \n",
    "        # Record epoch metrics\n",
    "        history.append(epoch_history)\n",
    "        \n",
    "        if (epoch + 1) % samples_per_epoch == 0:\n",
    "            # Generate outputs for fixed latent samples\n",
    "            samples[epoch] = model_gen(sample_noisev).data.numpy()\n",
    "            \n",
    "    return model_gen, model_dis, history, samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try on some training data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
